# ğŸš€ Ollama SDLC â€” CrewAI Multi-Agent Orchestration

<p align="center">
  <strong>7 AI Agents Ã— 3 Local LLMs Ã— 7 Pipeline Phases â€” 100% Offline on RTX 4090</strong>
</p>

<p align="center">
  <code>CrewAI</code> Â· <code>Ollama</code> Â· <code>Flask SSE</code> Â· <code>LiteLLM</code> Â· <code>Python 3.11+</code>
</p>

---

## ğŸ“‹ Overview

**Ollama SDLC** is a fully local, AI-powered **Software Development Lifecycle** automation framework. Give it a natural-language requirement and **7 specialized AI agents** collaborate to produce a complete software project â€” specification, code, code review, tests, documentation, DevOps configs, and a Streamlit UI â€” all running on **3 local Ollama LLMs** with zero cloud dependencies.

```
ğŸ“ "Build a todo list app"
        â†“
   ğŸ¤– 7 AI Agents collaborate
        â†“
   ğŸ“¦ Complete project output:
      â”œâ”€â”€ 01_specification.md
      â”œâ”€â”€ 02_code.py
      â”œâ”€â”€ 03_review.md
      â”œâ”€â”€ 04_tests.py
      â”œâ”€â”€ 05_documentation.md
      â”œâ”€â”€ 06_devops.md
      â””â”€â”€ 07_ui_app.py
```

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”’ **100% Local** | No API keys, no cloud â€” runs entirely on your GPU via Ollama |
| ğŸ¤– **7 Specialized Agents** | Each agent has a focused SDLC role, system prompt, and assigned model |
| ğŸ§  **3 Optimized LLMs** | `gpt-oss:20b`, `qwen3-coder:30b`, `devstral-small-2:24b` |
| ğŸ”„ **Iterative Code Review** | Developer â†” Reviewer loop (up to 3 rounds) until code is approved |
| ğŸ†” **UUID Traceability** | Every run and phase gets a UUID â€” full audit trail with manifests |
| ğŸŒ **Live Web Dashboard** | Flask + SSE real-time streaming of agent output |
| ï¿½ **History & Replay** | Browse past runs, inspect artifacts, view conversation logs |

---

## ğŸ“¸ Screenshots

### Dashboard â€” Launch New Pipeline
![Dashboard](docs/screenshots/01_dashboard.png)

### Pipeline History
![History](docs/screenshots/02_history.png)

### Run Detail â€” 7 Phase Artifacts with UUID Tracking
![Detail](docs/screenshots/03_detail.png)

> ğŸ“„ **Full Run Detail PDF:** [Run 71e86df3 â€” SDLC Dashboard.pdf](docs/Run%2071e86df3%20%E2%80%94%20SDLC%20Dashboard.pdf)

---

## ğŸ§  AI Agent Roster

| # | Agent | Role | Model | VRAM |
|---|-------|------|-------|------|
| 1 | **Requirement Analyst** | Transforms requirements into structured spec | `gpt-oss:20b` | 13 GB |
| 2 | **Senior Developer** | Generates production Python code | `qwen3-coder:30b` | 18 GB |
| 3 | **Code Reviewer** | Reviews code for bugs, security, PEP 8 | `devstral-small-2:24b` | 15 GB |
| 4 | **QA Engineer** | Creates comprehensive pytest test suite | `qwen3-coder:30b` | 18 GB |
| 5 | **Tech Writer** | Produces full documentation | `gpt-oss:20b` | 13 GB |
| 6 | **DevOps Agent** | Generates Dockerfile, CI/CD, configs | `gpt-oss:20b` | 13 GB |
| 7 | **UI Designer** | Creates Streamlit UI for the app | `gpt-oss:20b` | 13 GB |

---

## ğŸ”„ Pipeline Flow

```
User Requirement
      â”‚
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Requirement   â”‚ â”€â”€â”€ Analyst â†’ 01_specification.md
â”‚    Analysis      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Code          â”‚ â”€â”€â”€ Developer â†’ 02_code.py
â”‚    Generation    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Code Review   â”‚â—„â”€â”€â”€â–ºâ”‚  Revision    â”‚  (up to 3 rounds)
â”‚                  â”‚     â”‚  Loop        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼                    â”‚
         â”œâ”€â”€ 03_review.md â—„â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Testing       â”‚ â”€â”€â”€ QA Engineer â†’ 04_tests.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Documentation â”‚ â”€â”€â”€ Tech Writer â†’ 05_documentation.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. DevOps        â”‚ â”€â”€â”€ DevOps Agent â†’ 06_devops.md
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. UI Design     â”‚ â”€â”€â”€ UI Designer â†’ 07_ui_app.py
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‚ Project Structure

```
Ollama_SDLC_CrewAI_MultiAgent_Orchestration/
â”œâ”€â”€ flask_app/                  # Flask Web Dashboard (main entry)
â”‚   â”œâ”€â”€ app.py                  #   Flask server + SSE streaming
â”‚   â””â”€â”€ templates/              #   Jinja2 HTML templates
â”‚       â”œâ”€â”€ base.html           #     Layout base
â”‚       â”œâ”€â”€ index.html          #     Submit requirement form
â”‚       â”œâ”€â”€ live.html           #     Real-time agent streaming
â”‚       â”œâ”€â”€ history.html        #     Browse past runs
â”‚       â”œâ”€â”€ detail.html         #     Inspect run artifacts
â”‚       â””â”€â”€ 404.html            #     Error page
â”‚
â”œâ”€â”€ crewai_sdlc/                # CrewAI Agent & Pipeline Engine
â”‚   â”œâ”€â”€ __init__.py             #   Package init
â”‚   â”œâ”€â”€ config.py               #   LLM instances & agent mappings
â”‚   â””â”€â”€ crew.py                 #   Agents, tasks & crew assembly
â”‚
â”œâ”€â”€ crewai_output/              # Generated pipeline outputs
â”‚   â””â”€â”€ <uuid>/                 #   Per-run folder
â”‚       â”œâ”€â”€ 01_specification.md
â”‚       â”œâ”€â”€ 02_code.py
â”‚       â”œâ”€â”€ 03_review.md
â”‚       â”œâ”€â”€ 04_tests.py
â”‚       â”œâ”€â”€ 05_documentation.md
â”‚       â”œâ”€â”€ 06_devops.md
â”‚       â”œâ”€â”€ 07_ui_app.py
â”‚       â”œâ”€â”€ manifest.json
â”‚       â””â”€â”€ conversation_log.json
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ pyproject.toml              # PEP 621 project metadata
â”œâ”€â”€ .env.example                # Environment variable template
â”œâ”€â”€ .gitignore                  # Git ignore rules
â”œâ”€â”€ LICENSE                     # MIT License
â”œâ”€â”€ CONTRIBUTING.md             # Contribution guide
â””â”€â”€ PROJECT_DOCUMENTATION.md    # Comprehensive project docs
```

---

## ğŸ› ï¸ Prerequisites

- **Python 3.11+**
- **Ollama** installed and running (`ollama serve`)
- **NVIDIA GPU** with â‰¥ 24 GB VRAM (RTX 4090 recommended)
- Required Ollama models pulled:

```bash
ollama pull gpt-oss:20b
ollama pull qwen3-coder:30b
ollama pull devstral-small-2:24b
```

---

## âš™ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/nareshvrde5220/Ollama_SDLC_CrewAI_MultiAgent_Orchestration.git
cd Ollama_SDLC_CrewAI_MultiAgent_Orchestration

# Create and activate virtual environment
python -m venv ollama_env
ollama_env\Scripts\activate        # Windows
# source ollama_env/bin/activate   # Linux / macOS

# Install dependencies
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### Start the Dashboard

```bash
python flask_app/app.py
# Open http://localhost:5000 in your browser
```

### What You Can Do

- **Submit** a requirement via the web form (e.g. *"Build a REST API for a todo app"*)
- **Watch** 7 AI agents work in **real-time** via Server-Sent Events (SSE)
- **Browse** the **history** of all past pipeline runs
- **Inspect** individual run **artifacts** (spec, code, tests, docs, etc.)

---

## ğŸ“Š Output Artifacts

Each pipeline run generates 7 artifacts inside a UUID-named folder:

| File | Content |
|------|---------|
| `01_specification.md` | Structured requirements specification |
| `02_code.py` | Production Python implementation |
| `03_review.md` | Code review report with score & verdict |
| `04_tests.py` | pytest test suite |
| `05_documentation.md` | Full project documentation |
| `06_devops.md` | Dockerfile, docker-compose, CI/CD configs |
| `07_ui_app.py` | Streamlit UI application |

Plus `manifest.json` (run metadata) and `conversation_log.json` (full agent conversation history with UUIDs).

---

## ğŸ“ˆ Technical Specifications

| Spec | Value |
|------|-------|
| **GPU** | NVIDIA RTX 4090 (24 GB VRAM) |
| **Models** | 3 local Ollama LLMs (13â€“18 GB each) |
| **Max tokens** | 8,192 per agent call |
| **Temperature** | 0.2â€“0.3 (deterministic output) |
| **Review rounds** | Up to 3 iterations |
| **Request timeout** | 600s (10 min) per agent |
| **Web streaming** | Flask + Server-Sent Events |
| **Traceability** | UUID per run + per phase |

---

## ğŸ—ºï¸ Roadmap

- [ ] Multi-language support (JavaScript, Go, Rust)
- [ ] Parallel agent execution for independent phases
- [ ] Plugin system for custom agents
- [ ] Git integration for auto-committing artifacts
- [ ] Support for additional LLM backends (vLLM, llama.cpp server)
- [ ] Docker Compose for one-click deployment

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

<p align="center">
  Built with ğŸ¤– <strong>CrewAI</strong> + ğŸ¦™ <strong>Ollama</strong> â€” 100% local, 100% private
</p>
