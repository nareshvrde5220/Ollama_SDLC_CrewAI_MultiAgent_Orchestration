# ğŸš€ Olla SDLC â€” AI-Powered Software Development Lifecycle Framework

<p align="center">
  <strong>ğŸ¤– 7 AI Agents Ã— 3 Local LLMs Ã— 7 Pipeline Phases â€” 100% Offline on RTX 4090</strong>
</p>

<p align="center">
  <code>CrewAI</code> Â· <code>Ollama</code> Â· <code>Flask SSE</code> Â· <code>LiteLLM</code> Â· <code>Python 3.11+</code>
</p>

---

## ğŸ“‘ Table of Contents

| # | Section | Description |
|---|---------|-------------|
| 1 | [ğŸ¯ Project Overview](#-project-overview) | What this project does |
| 2 | [ğŸ—ï¸ System Architecture](#ï¸-system-architecture) | High-level architecture |
| 3 | [ğŸ§  AI Agent Roster](#-ai-agent-roster) | 7 specialized agents detailed |
| 4 | [ğŸ”„ Pipeline Flow](#-pipeline-flow) | 7-phase SDLC execution |
| 5 | [ğŸ“‚ Project Structure](#-project-structure) | File & folder layout |
| 6 | [âš™ï¸ Configuration Deep Dive](#ï¸-configuration-deep-dive) | Models, settings, mappings |
| 7 | [ğŸ–¥ï¸ Dual Execution Modes](#ï¸-dual-execution-modes) | CLI vs Flask Dashboard |
| 8 | [ğŸŒ Flask Web Dashboard](#-flask-web-dashboard) | SSE streaming UI details |
| 9 | [ğŸ”— API Reference](#-api-reference) | REST endpoints |
| 10 | [ğŸ“¦ Output Artifacts](#-output-artifacts) | What gets generated |
| 11 | [ğŸ§¬ UUID Traceability System](#-uuid-traceability-system) | Run tracking & manifests |
| 12 | [ğŸ› ï¸ Installation & Setup](#ï¸-installation--setup) | Step-by-step setup |
| 13 | [â–¶ï¸ Usage Guide](#ï¸-usage-guide) | How to run it |
| 14 | [ğŸ” Code Review Loop](#-code-review-loop) | Iterative review mechanism |
| 15 | [ğŸ“Š Technical Specifications](#-technical-specifications) | Hardware, models, limits |
| 16 | [ğŸ§© Module Deep Dive](#-module-deep-dive) | Every file explained |
| 17 | [ğŸ”® Data Flow Diagram](#-data-flow-diagram) | End-to-end data flow |
| 18 | [ğŸ›¡ï¸ Error Handling](#ï¸-error-handling) | Resilience patterns |
| 19 | [ğŸ“ˆ Performance Considerations](#-performance-considerations) | Optimization notes |
| 20 | [ğŸ—ºï¸ Roadmap](#ï¸-roadmap) | Future improvements |

---

## ğŸ¯ Project Overview

**Olla SDLC** is a **fully local, AI-powered Software Development Lifecycle (SDLC) automation framework** that transforms a natural-language software requirement into a complete, production-ready software project â€” including specification, code, tests, documentation, DevOps configs, and a Streamlit UI â€” all generated by **7 specialized AI agents** running on **3 local Ollama LLMs**.

### ğŸ’¡ What Problem Does It Solve?

Traditional software development requires a full team of specialists. This framework simulates an **entire engineering team** using AI:

```
ğŸ“ "Build a todo list app" 
        â†“
   ğŸ¤– 7 AI Agents collaborate
        â†“
   ğŸ“¦ Complete project with:
      â”œâ”€â”€ Specification document
      â”œâ”€â”€ Production Python code
      â”œâ”€â”€ Code review report
      â”œâ”€â”€ pytest test suite
      â”œâ”€â”€ Full documentation
      â”œâ”€â”€ Docker + CI/CD configs
      â””â”€â”€ Streamlit UI
```

### ğŸŒŸ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”’ **100% Local** | No API keys, no cloud â€” runs entirely on your GPU via Ollama |
| ğŸ¤– **7 Specialized Agents** | Each agent has a focused role, system prompt, and assigned model |
| ğŸ§  **3 Optimized LLMs** | `gpt-oss:20b`, `qwen3-coder:30b`, `devstral-small-2:24b` |
| ğŸ”„ **Iterative Code Review** | Developer â†” Reviewer loop (up to 3 rounds) until code is approved |
| ğŸ†” **UUID Traceability** | Every run, every phase gets a UUID â€” full audit trail |
| ğŸŒ **Live Web Dashboard** | Flask + SSE real-time streaming of agent output |
| ğŸ“Ÿ **CLI Mode** | Headless terminal execution for CI/CD integration |
| ğŸ“Š **History & Replay** | Browse past runs, inspect artifacts, view conversation logs |
| ğŸ—ï¸ **Dual Architecture** | Two implementations: raw Ollama API (`sdlc_agents/`) and CrewAI (`crewai_sdlc/`) |

---

## ğŸ—ï¸ System Architecture

### ğŸ” High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        USER INPUT LAYER                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  CLI Terminal â”‚          â”‚   Flask Web Dashboard (SSE)    â”‚      â”‚
â”‚   â”‚  run_sdlc.py â”‚          â”‚   flask_app/app.py :5000       â”‚      â”‚
â”‚   â”‚  run_crewai.. â”‚          â”‚   â”œâ”€â”€ index.html (submit)     â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚   â”œâ”€â”€ live.html  (streaming)   â”‚      â”‚
â”‚          â”‚                   â”‚   â”œâ”€â”€ history.html (browse)    â”‚      â”‚
â”‚          â”‚                   â”‚   â””â”€â”€ detail.html  (inspect)   â”‚      â”‚
â”‚          â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚
â”‚                     â–¼                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              ORCHESTRATION LAYER                              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚   sdlc_agents/       â”‚  â”‚   crewai_sdlc/               â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   orchestrator.py    â”‚  â”‚   crew.py                    â”‚  â”‚    â”‚
â”‚  â”‚  â”‚   (Raw API Mode)     â”‚  â”‚   (CrewAI Framework Mode)    â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â”‚             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                 AI AGENT LAYER (7 Agents)                     â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚    â”‚
â”‚  â”‚  â”‚ Analyst  â”‚ â”‚Developer â”‚ â”‚ Reviewer â”‚ â”‚ QA Eng   â”‚       â”‚    â”‚
â”‚  â”‚  â”‚ (gpt-oss)â”‚ â”‚(qwen3)  â”‚ â”‚(devstral)â”‚ â”‚(qwen3)   â”‚       â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚    â”‚
â”‚  â”‚  â”‚ Writer   â”‚ â”‚  DevOps  â”‚ â”‚ UI Desgn â”‚                     â”‚    â”‚
â”‚  â”‚  â”‚ (gpt-oss)â”‚ â”‚ (gpt-oss)â”‚ â”‚ (gpt-oss)â”‚                     â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                   LLM INFERENCE LAYER                         â”‚    â”‚
â”‚  â”‚           Ollama Server (http://localhost:11434)               â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚
â”‚  â”‚  â”‚ gpt-oss:20b â”‚ â”‚qwen3-coder   â”‚ â”‚devstral-small-2    â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ 13 GB VRAM  â”‚ â”‚:30b 18GB VRAMâ”‚ â”‚:24b 15 GB VRAM     â”‚    â”‚    â”‚
â”‚  â”‚  â”‚ Reasoning   â”‚ â”‚ Code Gen     â”‚ â”‚ Code Review         â”‚    â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚
â”‚  â”‚                 NVIDIA RTX 4090 (24 GB VRAM)                  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                  OUTPUT / PERSISTENCE LAYER                   â”‚    â”‚
â”‚  â”‚   crewai_output/<run_uuid>/                                   â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ manifest.json          (run metadata + UUID mapping)    â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ conversation_log.json  (agent conversation audit)       â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ 01_specification.md    (requirements spec)              â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ 02_code.py             (production Python code)         â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ 03_review.md           (code review report)             â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ 04_tests.py            (pytest test suite)              â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ 05_documentation.md    (README + docs)                  â”‚    â”‚
â”‚  â”‚   â”œâ”€â”€ 06_devops.md           (Docker + CI/CD)                 â”‚    â”‚
â”‚  â”‚   â””â”€â”€ 07_ui_app.py           (Streamlit UI)                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”€ Two Execution Architectures

This project provides **two parallel implementations** of the same pipeline:

| Aspect | `sdlc_agents/` (Raw Mode) | `crewai_sdlc/` (CrewAI Mode) |
|--------|---------------------------|-------------------------------|
| ğŸ”§ **Framework** | Direct Ollama REST API | CrewAI + LiteLLM |
| ğŸ“¡ **API Endpoint** | `/v1/chat/completions` | CrewAI's LLM abstraction |
| ğŸ”„ **Orchestration** | Manual `SDLCOrchestrator` class | `Crew(process=sequential)` |
| ğŸ’¬ **Conversation** | Manual history management | CrewAI's built-in context passing |
| ğŸ“Š **Callbacks** | None (polling) | `step_callback` + `task_callback` |
| ğŸŒ **Web Dashboard** | âŒ Not supported | âœ… Flask + SSE streaming |
| ğŸ“ **Entry Point** | `run_sdlc.py` | `run_crewai_sdlc.py` |

---

## ğŸ§  AI Agent Roster

### ğŸ“‹ The 7 Agents at a Glance

```
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  ğŸ§  AGENT ARMY â€” SDLC MULTI-AGENT FRAMEWORK                   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚                                                                â”‚
  â”‚  ğŸ“ Requirement Analyst â”€â”€â”€â†’ gpt-oss:20b (13GB)               â”‚
  â”‚  ğŸ’» Senior Developer    â”€â”€â”€â†’ qwen3-coder:30b (18GB)           â”‚
  â”‚  ğŸ” Code Reviewer       â”€â”€â”€â†’ devstral-small-2:24b (15GB)      â”‚
  â”‚  âœ… QA Engineer          â”€â”€â”€â†’ qwen3-coder:30b (18GB)           â”‚
  â”‚  ğŸ“– Tech Writer          â”€â”€â”€â†’ gpt-oss:20b (13GB)               â”‚
  â”‚  ğŸ³ DevOps Agent         â”€â”€â”€â†’ gpt-oss:20b (13GB)               â”‚
  â”‚  ğŸ¨ UI Designer          â”€â”€â”€â†’ gpt-oss:20b (13GB)               â”‚
  â”‚                                                                â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent 1: ğŸ“ Requirement Analyst

| Property | Value |
|----------|-------|
| **Model** | `gpt-oss:20b` (13 GB VRAM) |
| **Temperature** | 0.3 |
| **Max Iterations** | 1 |
| **Input** | Natural-language user requirement |
| **Output** | Structured Markdown specification |

**System Prompt Responsibilities:**
- ğŸ“Œ Extract Project Title & Overview
- ğŸ“Œ Enumerate Functional Requirements (numbered)
- ğŸ“Œ Define Non-Functional Requirements (performance, security)
- ğŸ“Œ Specify Input/Output contracts
- ğŸ“Œ List Constraints & Assumptions
- ğŸ“Œ Define Acceptance Criteria

**Why `gpt-oss:20b`?** â†’ Excels at reasoning, structured planning, and natural-language understanding. Ideal for converting vague descriptions into precise specifications.

---

### Agent 2: ğŸ’» Senior Developer

| Property | Value |
|----------|-------|
| **Model** | `qwen3-coder:30b` (18 GB VRAM) |
| **Temperature** | 0.2 |
| **Max Iterations** | 3 |
| **Input** | Specification from Analyst |
| **Output** | Complete Python source code |

**Code Generation Rules:**
- âœ… PEP 8 compliant
- âœ… Type hints on all function signatures
- âœ… Docstrings for all classes and functions
- âœ… Proper error handling (`try/except`)
- âœ… `__main__` guard for executable scripts
- âœ… Minimal imports

**Why `qwen3-coder:30b`?** â†’ MoE (Mixture of Experts) architecture with 30B parameters but only 3B active â€” massive context window (256K tokens), optimized for code generation.

---

### Agent 3: ğŸ” Code Reviewer

| Property | Value |
|----------|-------|
| **Model** | `devstral-small-2:24b` (15 GB VRAM) |
| **Temperature** | 0.3 |
| **Max Iterations** | 1 |
| **Input** | Code + Specification |
| **Output** | Structured review with verdict |

**Review Criteria:**
- ğŸ” Correctness & logic errors
- ğŸ” Security vulnerabilities (injection, auth, data exposure)
- ğŸ” Performance issues (complexity, memory, I/O)
- ğŸ” PEP 8 style compliance
- ğŸ” Error handling completeness
- ğŸ” Type hint correctness
- ğŸ” Edge cases & boundary conditions

**Review Output Format:**
```
1. Verdict: APPROVED / NEEDS_REVISION
2. Score: 1-10 (7+ = APPROVED)
3. Issues Found (severity: Critical/Major/Minor)
4. Specific Fixes (exact code changes)
5. Positive Aspects
```

**Why `devstral-small-2:24b`?** â†’ 384K context window, designed for multi-file code understanding and editing. Perfect for reviewing large codebases.

---

### Agent 4: âœ… QA Engineer

| Property | Value |
|----------|-------|
| **Model** | `qwen3-coder:30b` (18 GB VRAM) |
| **Temperature** | 0.2 |
| **Max Iterations** | 1 |
| **Input** | Code + Specification |
| **Output** | Complete pytest test suite |

**Testing Strategy:**
- ğŸ§ª Unit tests for all public functions/classes
- ğŸ§ª Integration tests for workflow paths
- ğŸ§ª Edge cases & boundary values
- ğŸ§ª Error paths & exception handling
- ğŸ§ª Parametrized tests with `@pytest.mark.parametrize`
- ğŸ§ª Fixtures for setup/teardown
- ğŸ§ª Target: >90% code coverage

---

### Agent 5: ğŸ“– Tech Writer

| Property | Value |
|----------|-------|
| **Model** | `gpt-oss:20b` (13 GB VRAM) |
| **Temperature** | 0.3 |
| **Max Iterations** | 1 |
| **Input** | Code + Specification |
| **Output** | Complete Markdown documentation |

**Documentation Sections:**
- ğŸ“„ README.md with installation & usage
- ğŸ“„ API reference with function signatures
- ğŸ“„ Architecture explanation
- ğŸ“„ Code examples
- ğŸ“„ Configuration guide
- ğŸ“„ Troubleshooting section

---

### Agent 6: ğŸ³ DevOps Agent

| Property | Value |
|----------|-------|
| **Model** | `gpt-oss:20b` (13 GB VRAM) |
| **Temperature** | 0.3 |
| **Max Iterations** | 1 |
| **Input** | Code + Specification |
| **Output** | Deployment configuration files |

**Generated DevOps Artifacts:**
- ğŸ³ `Dockerfile` â€” Multi-stage build, minimal image, non-root user
- ğŸ³ `docker-compose.yml` â€” Service orchestration
- ğŸ“‹ `requirements.txt` â€” Pinned dependencies
- âš¡ `.github/workflows/ci.yml` â€” GitHub Actions CI/CD
- ğŸ” `.env.example` â€” Environment variable template
- ğŸ©º Health checks & layer caching

---

### Agent 7: ğŸ¨ UI Designer

| Property | Value |
|----------|-------|
| **Model** | `gpt-oss:20b` (13 GB VRAM) |
| **Temperature** | 0.3 |
| **Max Iterations** | 1 |
| **Input** | Code + Specification |
| **Output** | Complete Streamlit application |

**UI Design Principles:**
- ğŸ¨ `st.columns`, `st.tabs`, `st.expander` for layout
- ğŸ¨ Page config (title, icon, wide layout)
- ğŸ¨ Sidebar navigation
- ğŸ¨ Loading spinners (`st.spinner`)
- ğŸ¨ Feedback messages (`st.success`, `st.error`, `st.warning`)
- ğŸ¨ Responsive and intuitive design

---

## ğŸ”„ Pipeline Flow

### ğŸ“Š Sequential 7-Phase Pipeline

```
                 USER REQUIREMENT (natural language)
                            â”‚
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ğŸ“ PHASE 1: REQUIREMENT ANALYSIS     â”‚
         â”‚  Agent: Requirement Analyst            â”‚
         â”‚  Model: gpt-oss:20b                    â”‚
         â”‚  Output: 01_specification.md           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ Specification
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ğŸ’» PHASE 2: CODE GENERATION          â”‚
         â”‚  Agent: Senior Developer               â”‚
         â”‚  Model: qwen3-coder:30b                â”‚
         â”‚  Output: 02_code.py                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ Python Code
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ğŸ” PHASE 3: CODE REVIEW (iterative)  â”‚â—„â”€â”€â”€â”€â”€â”€â”
         â”‚  Agent: Code Reviewer                  â”‚       â”‚
         â”‚  Model: devstral-small-2:24b           â”‚       â”‚
         â”‚  Output: 03_review.md                  â”‚       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
                            â”‚                             â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”                      â”‚
                     â”‚  APPROVED?  â”‚                      â”‚
                     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                      â”‚
                   YES â”‚          â”‚ NO (max 3 rounds)    â”‚
                       â”‚          â””â”€â”€â”€â”€ ğŸ’» Developer â”€â”€â”€â”€â”˜
                       â”‚                 (revises code)
                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  âœ… PHASE 4: TEST GENERATION           â”‚
         â”‚  Agent: QA Engineer                    â”‚
         â”‚  Model: qwen3-coder:30b                â”‚
         â”‚  Output: 04_tests.py                   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ pytest suite
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ğŸ“– PHASE 5: DOCUMENTATION             â”‚
         â”‚  Agent: Tech Writer                    â”‚
         â”‚  Model: gpt-oss:20b                    â”‚
         â”‚  Output: 05_documentation.md           â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ Docs
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ğŸ³ PHASE 6: DEVOPS CONFIGURATION     â”‚
         â”‚  Agent: DevOps Agent                   â”‚
         â”‚  Model: gpt-oss:20b                    â”‚
         â”‚  Output: 06_devops.md                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ Docker + CI/CD
                            â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ğŸ¨ PHASE 7: UI DESIGN                â”‚
         â”‚  Agent: UI Designer                    â”‚
         â”‚  Model: gpt-oss:20b                    â”‚
         â”‚  Output: 07_ui_app.py                  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                  âœ… PIPELINE COMPLETE
                  ğŸ“¦ All 7 artifacts saved
                  ğŸ†” manifest.json generated
                  ğŸ“‹ conversation_log.json saved
```

### â±ï¸ Phase Timing (Typical)

| Phase | Agent | Model | Est. Time |
|-------|-------|-------|-----------|
| 1. Requirement Analysis | Requirement Analyst | gpt-oss:20b | ~30-60s |
| 2. Code Generation | Senior Developer | qwen3-coder:30b | ~60-120s |
| 3. Code Review (Ã—1-3) | Code Reviewer + Developer | devstral + qwen3 | ~60-180s |
| 4. Test Generation | QA Engineer | qwen3-coder:30b | ~60-120s |
| 5. Documentation | Tech Writer | gpt-oss:20b | ~30-60s |
| 6. DevOps Config | DevOps Agent | gpt-oss:20b | ~30-60s |
| 7. UI Design | UI Designer | gpt-oss:20b | ~30-60s |
| **Total** | | | **~5-12 min** |

---

## ğŸ“‚ Project Structure

```
Olla_SDLC_Claude_Copilot/
â”‚
â”œâ”€â”€ ğŸ“„ run_crewai_sdlc.py          # ğŸ”µ CLI entry point (CrewAI mode)
â”œâ”€â”€ ğŸ“„ run_sdlc.py                  # ğŸŸ¢ CLI entry point (Raw API mode)
â”‚
â”œâ”€â”€ ğŸ“ crewai_sdlc/                 # ğŸ”µ CrewAI-based implementation
â”‚   â”œâ”€â”€ __init__.py                 #     Package exports
â”‚   â”œâ”€â”€ config.py                   #     LLM instances, agentâ†’model mapping
â”‚   â””â”€â”€ crew.py                     #     Agents, Tasks, Crew, Pipeline runner
â”‚
â”œâ”€â”€ ğŸ“ sdlc_agents/                 # ğŸŸ¢ Raw Ollama API implementation
â”‚   â”œâ”€â”€ __init__.py                 #     Package exports
â”‚   â”œâ”€â”€ config.py                   #     Model names, settings
â”‚   â”œâ”€â”€ base_agent.py               #     BaseAgent class (HTTP client)
â”‚   â”œâ”€â”€ agents.py                   #     7 specialized agent classes
â”‚   â””â”€â”€ orchestrator.py             #     SDLCOrchestrator pipeline
â”‚
â”œâ”€â”€ ğŸ“ flask_app/                   # ğŸŒ Web Dashboard
â”‚   â”œâ”€â”€ app.py                      #     Flask server + SSE + OutputInterceptor
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ base.html               #     Dark glass theme layout
â”‚       â”œâ”€â”€ index.html              #     Pipeline submission form
â”‚       â”œâ”€â”€ live.html               #     Real-time SSE streaming view
â”‚       â”œâ”€â”€ history.html            #     Past runs browser
â”‚       â”œâ”€â”€ detail.html             #     Run artifact inspector
â”‚       â””â”€â”€ 404.html                #     Error page
â”‚
â”œâ”€â”€ ğŸ“ crewai_output/               # ğŸ“¦ Generated output (per-run UUID folders)
â”‚   â”œâ”€â”€ manifest.json               #     Global manifest
â”‚   â”œâ”€â”€ conversation_log.json       #     Global conversation log
â”‚   â”œâ”€â”€ <uuid-1>/                   #     Run 1 artifacts
â”‚   â”‚   â”œâ”€â”€ manifest.json
â”‚   â”‚   â”œâ”€â”€ conversation_log.json
â”‚   â”‚   â”œâ”€â”€ 01_specification.md
â”‚   â”‚   â”œâ”€â”€ 02_code.py
â”‚   â”‚   â”œâ”€â”€ 03_review.md
â”‚   â”‚   â”œâ”€â”€ 04_tests.py
â”‚   â”‚   â”œâ”€â”€ 05_documentation.md
â”‚   â”‚   â”œâ”€â”€ 06_devops.md
â”‚   â”‚   â””â”€â”€ 07_ui_app.py
â”‚   â””â”€â”€ <uuid-2>/                   #     Run 2 artifacts
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ“ output/                      # ğŸ“¦ Output for raw mode (flat)
â”‚
â””â”€â”€ ğŸ“ ollama_env/                  # ğŸ Python virtual environment
    â”œâ”€â”€ pyvenv.cfg
    â”œâ”€â”€ Scripts/                    #     Python executables
    â””â”€â”€ Lib/site-packages/          #     Installed packages
```

---

## âš™ï¸ Configuration Deep Dive

### ğŸ”§ CrewAI Mode (`crewai_sdlc/config.py`)

```python
# â”€â”€ Ollama Server â”€â”€
OLLAMA_BASE_URL = "http://localhost:11434"

# â”€â”€ LLM Instances (CrewAI LLM class via LiteLLM) â”€â”€
LLM_PLANNER  = LLM(model="ollama/gpt-oss:20b",          temperature=0.3)
LLM_CODER    = LLM(model="ollama/qwen3-coder:30b",       temperature=0.2)
LLM_REVIEWER = LLM(model="ollama/devstral-small-2:24b",  temperature=0.3)

# â”€â”€ Agent â†’ LLM Mapping â”€â”€
AGENT_LLMS = {
    "orchestrator":        LLM_PLANNER,   # Planning & coordination
    "requirement_analyst": LLM_PLANNER,   # Specification writing
    "senior_developer":    LLM_CODER,     # Code generation
    "code_reviewer":       LLM_REVIEWER,  # Code analysis
    "tech_writer":         LLM_PLANNER,   # Documentation
    "qa_engineer":         LLM_CODER,     # Test generation
    "devops_agent":        LLM_PLANNER,   # DevOps configs
    "ui_designer":         LLM_PLANNER,   # Streamlit UI
}

REVIEW_MAX_ITERATIONS = 3   # Max review-revision cycles
OUTPUT_DIR = "output"        # Default output directory
```

### ğŸ”§ Raw API Mode (`sdlc_agents/config.py`)

```python
OLLAMA_BASE_URL = "http://localhost:11434"

# â”€â”€ Agent â†’ Model (string names, direct Ollama API) â”€â”€
AGENT_MODELS = {
    "orchestrator":        "gpt-oss:20b",
    "requirement_analyst": "gpt-oss:20b",
    "senior_developer":    "qwen3-coder:30b",
    "code_reviewer":       "devstral-small-2:24b",
    "tech_writer":         "gpt-oss:20b",
    "qa_engineer":         "qwen3-coder:30b",
    "devops_agent":        "gpt-oss:20b",
    "ui_designer":         "gpt-oss:20b",
}

MAX_TOKENS = 8192           # Max generation tokens per call
TEMPERATURE = 0.3           # Low for deterministic output
REVIEW_MAX_ITERATIONS = 3   # Review loop limit
REQUEST_TIMEOUT = 600       # 10 min timeout per call
```

### ğŸ§  Model Selection Rationale

| Model | Size | VRAM | Strengths | Assigned To |
|-------|------|------|-----------|-------------|
| `gpt-oss:20b` | 20B params | ~13 GB | Reasoning, planning, structured writing | Analyst, Writer, DevOps, UI |
| `qwen3-coder:30b` | 30B (MoE 3B active) | ~18 GB | Code generation, 256K context | Developer, QA |
| `devstral-small-2:24b` | 24B params | ~15 GB | Code review, multi-file editing, 384K context | Code Reviewer |

> ğŸ’¡ **Note:** Only one model is loaded in VRAM at a time. Ollama automatically swaps models as different agents are invoked. This allows running all 3 models on a single 24GB GPU.

---

## ğŸ–¥ï¸ Dual Execution Modes

### Mode 1: ğŸ“Ÿ CLI â€” `run_crewai_sdlc.py` (CrewAI)

```bash
# Quick run with inline requirement
python run_crewai_sdlc.py -r "Build a simple calculator"

# Load requirement from file
python run_crewai_sdlc.py -f requirements.txt -o my_output

# Interactive mode (type requirement, end with "END")
python run_crewai_sdlc.py
```

**CLI Startup Sequence:**
1. ğŸ–¨ï¸ Print framework header (agent â†’ model mapping)
2. ğŸ”Œ Check Ollama server connectivity
3. ğŸ§  Verify all required models are installed
4. ğŸ“ Accept requirement (arg / file / interactive)
5. ğŸš€ Execute 7-phase pipeline
6. ğŸ“¦ Save artifacts to `crewai_output/<uuid>/`

### Mode 2: ğŸŒ Flask Dashboard â€” `flask_app/app.py`

```bash
python flask_app/app.py
# â†’ http://localhost:5000
```

**Dashboard Pages:**
| Route | Page | Description |
|-------|------|-------------|
| `/` | ğŸ  Index | Submit new requirement, view agent roster |
| `/live/<run_id>` | ğŸ“¡ Live | Real-time SSE streaming of agent output |
| `/history` | ğŸ“‹ History | Browse all past pipeline runs |
| `/detail/<run_id>` | ğŸ” Detail | Inspect artifacts, conversation logs |

---

## ğŸŒ Flask Web Dashboard

### ğŸ—ï¸ Dashboard Architecture

```
Browser                     Flask Server (Port 5000)
  â”‚                                â”‚
  â”‚  POST /start                   â”‚
  â”‚  {"requirement": "..."}        â”‚
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ â†’ Spawn pipeline thread
  â”‚  {"run_id": "<uuid>"}          â”‚
  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                                â”‚
  â”‚  GET /stream/<run_id>          â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Accept: text/event-stream     â”‚   â”‚  Pipeline Thread     â”‚
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”‚                                â”‚   â”‚  â”‚ CrewAI Crew    â”‚  â”‚
  â”‚  SSE: pipeline_start           â”‚   â”‚  â”‚ kickoff()      â”‚  â”‚
  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  â”‚  SSE: phase_start (phase 0)    â”‚   â”‚          â”‚           â”‚
  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚  stdout/stderr       â”‚
  â”‚  SSE: agent_output (chunks)    â”‚   â”‚  intercepted via     â”‚
  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚  OutputInterceptor   â”‚
  â”‚  SSE: phase_complete (phase 0) â”‚   â”‚          â”‚           â”‚
  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚  task_callback()     â”‚
  â”‚  SSE: phase_start (phase 1)    â”‚   â”‚  fires on each      â”‚
  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚   â”‚  task completion     â”‚
  â”‚  ...                            â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚  SSE: pipeline_complete         â”‚
  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚  SSE: done                      â”‚
  â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
```

### ğŸ”„ SSE Event Types

| Event | Payload | Description |
|-------|---------|-------------|
| `pipeline_start` | `run_id`, `timestamp`, `phases[]` | Pipeline initialized |
| `status` | `message` | Status update text |
| `phase_start` | `phase_index`, `agent`, `model`, `icon`, `color` | Phase begins |
| `agent_output` | `phase_index`, `text` | Real-time agent text (chunked) |
| `phase_complete` | `phase_index`, `content`, `file` | Phase finished with output |
| `pipeline_complete` | `run_id`, `elapsed_seconds`, `files_count` | All phases done |
| `error` | `message` | Error occurred |
| `heartbeat` | â€” | Keep-alive (every 120s) |
| `done` | â€” | Stream closed |

### ğŸ­ OutputInterceptor â€” Real-Time Capture

The `OutputInterceptor` is a **thread-aware stdout/stderr replacement** that captures all agent output in real-time:

```python
class OutputInterceptor(io.TextIOBase):
    """
    Replaces sys.stdout/sys.stderr BEFORE any CrewAI imports.
    
    For each write():
      1. Pass through to original terminal (always visible)
      2. Route to thread-specific PipelineHandler (if registered)
      3. PipelineHandler pushes SSE events to the run's Queue
    """
```

**Key Design Decisions:**
- ğŸ”’ **Installed before any import** â€” ensures CrewAI's Rich Console uses our interceptor
- ğŸ§µ **Thread-aware** â€” each pipeline thread registers its own handler
- ğŸ§¹ **ANSI cleanup** â€” strips color codes, replaces Unicode box chars
- â±ï¸ **Buffered flush** â€” aggregates chunks for 250ms before pushing SSE

---

## ğŸ”— API Reference

### REST Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Render index page |
| `POST` | `/start` | Start new pipeline run |
| `GET` | `/stream/<run_id>` | SSE event stream |
| `GET` | `/live/<run_id>` | Live streaming page |
| `GET` | `/history` | Past runs page |
| `GET` | `/detail/<run_id>` | Run detail page |
| `GET` | `/api/runs` | JSON list of all runs |
| `GET` | `/api/run/<run_id>` | JSON manifest for a run |
| `GET` | `/api/artifact/<run_id>/<filename>` | Raw artifact content |

### `POST /start` â€” Start Pipeline

**Request:**
```json
{
  "requirement": "Build a REST API for a todo app"
}
```

**Response:**
```json
{
  "run_id": "71e86df3-1dad-4144-b2e2-985bdc3010b0"
}
```

### `GET /api/run/<run_id>` â€” Run Manifest

**Response:**
```json
{
  "run_id": "71e86df3-...",
  "timestamp": "2026-02-07T10:30:00+00:00",
  "elapsed_seconds": 420.5,
  "framework": "CrewAI + LiteLLM + Ollama",
  "user_requirement": "Build a simple todo list...",
  "status": "completed",
  "phases": [
    {
      "phase_id": "a1b2c3d4-...",
      "phase_number": 1,
      "phase_name": "requirement_analysis",
      "agent": "Requirement Analyst",
      "model": "ollama/gpt-oss:20b",
      "output_file": "01_specification.md"
    }
  ],
  "conversations": [...]
}
```

---

## ğŸ“¦ Output Artifacts

Each pipeline run generates **7 artifacts + 2 metadata files** in a UUID-named folder:

```
crewai_output/<run-uuid>/
â”œâ”€â”€ ğŸ“‹ manifest.json            # Run metadata, phase UUIDs, timing
â”œâ”€â”€ ğŸ’¬ conversation_log.json    # Agent conversation audit trail
â”œâ”€â”€ ğŸ“ 01_specification.md      # Structured requirement specification
â”œâ”€â”€ ğŸ’» 02_code.py               # Production-ready Python code
â”œâ”€â”€ ğŸ” 03_review.md             # Code review report (verdict + score)
â”œâ”€â”€ ğŸ§ª 04_tests.py              # pytest test suite
â”œâ”€â”€ ğŸ“– 05_documentation.md      # README, API docs, usage examples
â”œâ”€â”€ ğŸ³ 06_devops.md             # Dockerfile, docker-compose, CI/CD
â””â”€â”€ ğŸ¨ 07_ui_app.py             # Streamlit UI application
```

### Artifact Details

| # | File | Format | Generated By | Contains |
|---|------|--------|--------------|----------|
| 1 | `01_specification.md` | Markdown | Requirement Analyst | Title, overview, functional/non-functional requirements, I/O spec, acceptance criteria |
| 2 | `02_code.py` | Python | Senior Developer | Production code with type hints, docstrings, error handling, `__main__` guard |
| 3 | `03_review.md` | Markdown | Code Reviewer | Verdict (APPROVED/NEEDS_REVISION), score (1-10), issues, fixes, positives |
| 4 | `04_tests.py` | Python | QA Engineer | pytest suite with unit tests, integration tests, fixtures, parametrize |
| 5 | `05_documentation.md` | Markdown | Tech Writer | README, installation, usage, API reference, config guide, troubleshooting |
| 6 | `06_devops.md` | Markdown | DevOps Agent | Dockerfile, docker-compose.yml, requirements.txt, GitHub Actions CI/CD |
| 7 | `07_ui_app.py` | Python | UI Designer | Streamlit app with sidebar, tabs, columns, spinners, feedback |

---

## ğŸ§¬ UUID Traceability System

Every pipeline run and every phase is tracked with **UUIDs** for full auditability.

### ğŸ†” UUID Hierarchy

```
Pipeline Run UUID (run_id)
â”œâ”€â”€ Phase 1 UUID (requirement_analysis)
â”œâ”€â”€ Phase 2 UUID (code_generation)
â”œâ”€â”€ Phase 3 UUID (code_review)
â”œâ”€â”€ Phase 4 UUID (test_generation)
â”œâ”€â”€ Phase 5 UUID (documentation)
â”œâ”€â”€ Phase 6 UUID (devops_configuration)
â””â”€â”€ Phase 7 UUID (ui_design)
```

### ğŸ“‹ Manifest Structure

```json
{
  "run_id": "71e86df3-1dad-4144-b2e2-985bdc3010b0",
  "timestamp": "2026-02-07T10:30:00.123456+00:00",
  "elapsed_seconds": 420.5,
  "framework": "CrewAI + LiteLLM + Ollama",
  "user_requirement": "Build a simple todo list application...",
  "status": "completed",
  "phases": [
    {
      "phase_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
      "phase_number": 1,
      "phase_name": "requirement_analysis",
      "agent": "Requirement Analyst",
      "model": "ollama/gpt-oss:20b",
      "output_file": "01_specification.md"
    }
  ],
  "conversations": [
    {
      "phase_id": "a1b2c3d4-...",
      "phase": "requirement_analysis",
      "agent": "Requirement Analyst",
      "model": "ollama/gpt-oss:20b",
      "input_context": "Build a simple todo list...",
      "output_length": 2847,
      "output_file": "01_specification.md"
    }
  ]
}
```

### ğŸ”– Phase UUID in Artifact Files

Each saved artifact includes a UUID header:

**Python files:**
```python
# Phase UUID: a1b2c3d4-e5f6-7890-abcd-ef1234567890

def main():
    ...
```

**Markdown files:**
```markdown
<!-- Phase UUID: a1b2c3d4-e5f6-7890-abcd-ef1234567890 -->

# Specification Document
...
```

---

## ğŸ› ï¸ Installation & Setup

### ğŸ“‹ Prerequisites

| Requirement | Version | Purpose |
|-------------|---------|---------|
| ğŸ Python | 3.11+ | Runtime |
| ğŸ§  Ollama | Latest | Local LLM inference server |
| ğŸ® NVIDIA GPU | RTX 4090 (24 GB) | LLM inference acceleration |
| ğŸ§ CUDA | 12.x | GPU compute |

### Step 1: ğŸ“¥ Clone the Repository

```bash
git clone https://github.com/YourUser/Olla_SDLC_Claude_Copilot.git
cd Olla_SDLC_Claude_Copilot
```

### Step 2: ğŸ Set Up Python Environment

```bash
python -m venv ollama_env

# Windows
ollama_env\Scripts\activate

# Linux/Mac
source ollama_env/bin/activate
```

### Step 3: ğŸ“¦ Install Dependencies

```bash
pip install crewai litellm flask requests
```

### Step 4: ğŸ§  Install Ollama & Pull Models

```bash
# Install Ollama (https://ollama.com)
# Then pull the 3 required models:

ollama pull gpt-oss:20b            # ~13 GB â€” reasoning & planning
ollama pull qwen3-coder:30b        # ~18 GB â€” code generation (MoE)
ollama pull devstral-small-2:24b   # ~15 GB â€” code review
```

### Step 5: ğŸš€ Start Ollama Server

```bash
ollama serve
# Listens on http://localhost:11434
```

### Step 6: âœ… Verify Setup

```bash
python -c "from crewai_sdlc.crew import run_sdlc_pipeline; print('âœ… Import OK')"
```

---

## â–¶ï¸ Usage Guide

### ğŸ”µ CrewAI CLI Mode

```bash
# Option A: Inline requirement
python run_crewai_sdlc.py -r "Build a REST API for managing books"

# Option B: From file
echo "Build a weather dashboard" > req.txt
python run_crewai_sdlc.py -f req.txt -o my_output

# Option C: Interactive
python run_crewai_sdlc.py
# Type your requirement, then type END
```

### ğŸŸ¢ Raw API CLI Mode

```bash
python run_sdlc.py -r "Build a simple calculator"
python run_sdlc.py -f requirements.txt -o output
```

### ğŸŒ Web Dashboard Mode

```bash
# Start the Flask server
python flask_app/app.py

# Open browser â†’ http://localhost:5000
# 1. Type requirement in the form
# 2. Click "Start Pipeline"
# 3. Watch real-time agent output on /live/<run_id>
# 4. Browse history at /history
# 5. Inspect artifacts at /detail/<run_id>
```

### ğŸ“Š Viewing Results

```bash
# List generated artifacts
ls crewai_output/<run-uuid>/

# View specification
cat crewai_output/<run-uuid>/01_specification.md

# Run generated tests
pytest crewai_output/<run-uuid>/04_tests.py

# Run generated UI
streamlit run crewai_output/<run-uuid>/07_ui_app.py
```

---

## ğŸ” Code Review Loop

The **Code Review** phase (Phase 3) implements an **iterative review-revision loop** â€” the only non-linear step in the pipeline:

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ğŸ’» Senior Developer    â”‚
                    â”‚  generates/revises code â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”Œâ”€â”€â”€â”€â–ºâ”‚  ğŸ” Code Reviewer      â”‚
              â”‚     â”‚  reviews code           â”‚
              â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                 â”‚
              â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
              â”‚          â”‚  Verdict?   â”‚
              â”‚          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
              â”‚         APPROVED â”‚ NEEDS_REVISION
              â”‚            â”‚    â”‚
              â”‚            â”‚    â””â”€â”€â”€â”€â”€â”€â”
              â”‚            â”‚           â”‚
              â”‚            â”‚    Iteration < 3?
              â”‚            â”‚     YES â”‚  NO
              â”‚            â”‚         â”‚   â”‚
              â”‚            â–¼         â”‚   â–¼
              â”‚      âœ… Continue     â”‚  âš ï¸ Continue
              â”‚      to Phase 4     â”‚  (use latest code)
              â”‚                     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  Developer revises
                  based on feedback
```

### ğŸ”‘ Approval Rules

```python
def _is_approved(review):
    # Rule 1: Explicit "APPROVED" without "NEEDS_REVISION"
    if "APPROVED" in review.upper() and "NEEDS_REVISION" not in review.upper():
        return True
    
    # Rule 2: Score >= 7
    score_match = re.search(r"score[:\s]*(\d+)", review, re.IGNORECASE)
    if score_match and int(score_match.group(1)) >= 7:
        return True
    
    return False
```

---

## ğŸ“Š Technical Specifications

### ğŸ–¥ï¸ Current Development System Configuration

> ğŸ”¥ **This project is developed and tested on the following workstation:**

#### ğŸ® GPU â€” NVIDIA GeForce RTX 4090

| Spec | Value |
|------|-------|
| ğŸ·ï¸ **Model** | NVIDIA GeForce RTX 4090 |
| ğŸ§  **VRAM** | 24,564 MiB (24 GB GDDR6X) |
| âš¡ **TDP / Power Limit** | 450 W |
| ğŸ”Œ **PCIe** | Gen 1 Ã— 16 lanes |
| ğŸ—ï¸ **CUDA Cores** | 16,384 |
| ğŸ§® **Compute Capability** | 8.9 (Ada Lovelace) |
| ğŸ•¹ï¸ **Max GPU Clock** | 3,105 MHz |
| ğŸ“¦ **VBIOS** | 95.02.3c.80.9a |
| ğŸ› ï¸ **Driver Version** | 581.57 |
| ğŸ”§ **CUDA Version** | 13.0 |

#### ğŸ§  CPU â€” Intel Core i9-14900F

| Spec | Value |
|------|-------|
| ğŸ·ï¸ **Model** | IntelÂ® Coreâ„¢ i9-14900F |
| ğŸ”¢ **Cores / Threads** | 24 Cores / 32 Threads |
| âš¡ **Architecture** | Raptor Lake Refresh (P-cores + E-cores) |
| ğŸ•¹ï¸ **Base Clock** | 2.0 GHz (P-cores boost to 5.8 GHz) |

#### ğŸ§® RAM â€” 64 GB DDR5

| Spec | Value |
|------|-------|
| ğŸ“Š **Total Memory** | 64 GB DDR5 |
| ğŸ­ **Manufacturer** | Corsair |
| ğŸ“ **Configuration** | 2 Ã— 32 GB (Dual Channel) |
| âš¡ **Speed** | 4,800 MHz |

#### ğŸ–¥ï¸ Motherboard & Storage

| Component | Details |
|-----------|---------|
| ğŸ”§ **Motherboard** | Gigabyte Z790 D AX |
| ğŸ’¾ **Storage** | WD Blue SN580 2TB NVMe SSD |

#### ğŸ’» Operating System & Software Stack

| Component | Version |
|-----------|---------|
| ğŸªŸ **OS** | Microsoft Windows 11 Pro (64-bit) |
| ğŸ“¦ **Build** | 22631 |
| ğŸ **Python** | 3.12.4 |
| ğŸ¤– **CrewAI** | 1.9.3 |
| ğŸŒ **Flask** | 3.1.2 |
| ğŸ§  **Ollama** | Latest |
| ğŸ› ï¸ **NVIDIA Driver** | 581.57 |
| ğŸ”§ **CUDA** | 13.0 |

#### ğŸ“Š VRAM Usage Per Phase (Real-World)

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  ğŸ“Š VRAM ALLOCATION ON RTX 4090 (24 GB Total)                 â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚                                                                â”‚
 â”‚  gpt-oss:20b (Analyst/Writer/DevOps/UI)                       â”‚
 â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘  ~13 GB / 24 GB (54%)    â”‚
 â”‚                                                                â”‚
 â”‚  qwen3-coder:30b (Developer/QA)                                â”‚
 â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  ~18 GB / 24 GB (75%)   â”‚
 â”‚                                                                â”‚
 â”‚  devstral-small-2:24b (Reviewer)                               â”‚
 â”‚  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  ~15 GB / 24 GB (63%)    â”‚
 â”‚                                                                â”‚
 â”‚  ğŸ’¡ Only ONE model loaded at a time â€” Ollama auto-swaps       â”‚
 â”‚  â±ï¸ Model swap time: ~3-8 seconds                              â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ”¥ System At-a-Glance

```
 â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
 â•‘              ğŸ–¥ï¸ DEVELOPMENT WORKSTATION SPEC                  â•‘
 â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
 â•‘                                                               â•‘
 â•‘  CPU    : Intel Core i9-14900F (24C/32T @ 5.8 GHz)           â•‘
 â•‘  GPU    : NVIDIA RTX 4090 24GB GDDR6X (Ada Lovelace)         â•‘
 â•‘  RAM    : 64 GB DDR5-4800 (2Ã—32GB Corsair)                   â•‘
 â•‘  SSD    : WD Blue SN580 2TB NVMe                              â•‘
 â•‘  MOBO   : Gigabyte Z790 D AX                                  â•‘
 â•‘  OS     : Windows 11 Pro 64-bit (Build 22631)                 â•‘
 â•‘                                                               â•‘
 â•‘  NVIDIA : Driver 581.57 | CUDA 13.0 | Compute 8.9            â•‘
 â•‘  Python : 3.12.4 | CrewAI 1.9.3 | Flask 3.1.2                â•‘
 â•‘  LLM    : Ollama (localhost:11434)                             â•‘
 â•‘                                                               â•‘
 â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### ğŸ–¥ï¸ Hardware Requirements (General)

| Component | Minimum | Recommended | **This System** |
|-----------|---------|-------------|-----------------|
| GPU | NVIDIA RTX 3090 (24 GB) | NVIDIA RTX 4090 (24 GB) | âœ… RTX 4090 24 GB |
| RAM | 32 GB | 64 GB | âœ… 64 GB DDR5 |
| Disk | 50 GB free | 100 GB free | âœ… 2 TB NVMe |
| CPU | 8 cores | 16+ cores | âœ… 24C/32T i9-14900F |
| OS | Windows 10 / Linux | Windows 11 / Ubuntu 22+ | âœ… Windows 11 Pro |

### ğŸ§  Model Specifications

| Model | Parameters | VRAM Usage | Context Window | Architecture | Best For |
|-------|-----------|------------|----------------|--------------|----------|
| `gpt-oss:20b` | 20B | ~13 GB | Standard | Dense | Reasoning, planning |
| `qwen3-coder:30b` | 30B total, 3B active | ~18 GB | 256K tokens | MoE (Mixture of Experts) | Code generation |
| `devstral-small-2:24b` | 24B | ~15 GB | 384K tokens | Dense | Code review, editing |

### ğŸŒ Network Configuration

| Service | Protocol | Port | URL |
|---------|----------|------|-----|
| Ollama Server | HTTP | 11434 | `http://localhost:11434` |
| Flask Dashboard | HTTP | 5000 | `http://localhost:5000` |
| Ollama API (OpenAI compat) | HTTP | 11434 | `http://localhost:11434/v1/chat/completions` |
| Ollama API (native) | HTTP | 11434 | `http://localhost:11434/api/chat` |

---

## ğŸ§© Module Deep Dive

### ğŸ“ `crewai_sdlc/config.py` â€” LLM Configuration

**Purpose:** Instantiates CrewAI `LLM` objects with Ollama backend and maps them to agent roles.

**Key Constants:**
- `LLM_PLANNER` â€” gpt-oss:20b instance (temp=0.3)
- `LLM_CODER` â€” qwen3-coder:30b instance (temp=0.2)
- `LLM_REVIEWER` â€” devstral-small-2:24b instance (temp=0.3)
- `AGENT_LLMS` â€” dict mapping role keys â†’ LLM instances

---

### ğŸ“ `crewai_sdlc/crew.py` â€” CrewAI Pipeline Engine

**Purpose:** The heart of the CrewAI implementation. Contains 534 lines defining agents, tasks, crew assembly, and the pipeline runner.

**Key Functions:**

| Function | Description |
|----------|-------------|
| `create_agents()` | Instantiates 7 CrewAI `Agent` objects with roles, goals, backstories, LLMs |
| `create_tasks(agents, requirement)` | Creates 7 `Task` objects with descriptions, expected outputs, and context dependencies |
| `build_sdlc_crew(requirement)` | Assembles `Crew` with sequential process, optional callbacks |
| `extract_code(response)` | Strips markdown code fences from LLM output |
| `save_artifact(output_dir, filename, content, phase_id)` | Saves artifact with UUID header |
| `run_sdlc_pipeline(requirement, output_dir)` | Full pipeline: UUID â†’ crew.kickoff() â†’ save â†’ manifest |

**Task Context Dependencies:**
```
analyze_task  â†’ (no context, receives raw requirement)
develop_task  â†’ depends on [analyze_task]
review_task   â†’ depends on [analyze_task, develop_task]
test_task     â†’ depends on [analyze_task, develop_task]
doc_task      â†’ depends on [analyze_task, develop_task]
devops_task   â†’ depends on [analyze_task, develop_task]
ui_task       â†’ depends on [analyze_task, develop_task]
```

---

### ğŸ“ `sdlc_agents/base_agent.py` â€” HTTP Agent Client

**Purpose:** Base class for all raw-mode agents. Implements conversation management and Ollama API calls.

**Key Features:**
- ğŸ’¬ Maintains conversation history (system prompt + messages)
- ğŸ“¡ Calls `/v1/chat/completions` (OpenAI-compatible endpoint)
- â±ï¸ 600s timeout per request
- ğŸ”„ Automatic error handling for connection/timeout failures

```python
class BaseAgent:
    def __init__(self, name, role, model, system_prompt)
    def chat(self, user_message, temperature=None) â†’ str
    def reset(self)  # Clear conversation history
```

---

### ğŸ“ `sdlc_agents/orchestrator.py` â€” Manual Pipeline

**Purpose:** Manually coordinates all 7 agents through the pipeline without CrewAI.

**Key Methods:**
- `run(user_requirement)` â€” Executes all 7 phases sequentially
- `_extract_code()` â€” Strips markdown code fences
- `_is_approved()` â€” Checks review verdict/score
- `_save()` â€” Writes artifacts to disk
- `_phase()` / `_banner()` â€” Terminal UI formatting

---

### ğŸ“ `flask_app/app.py` â€” Web Dashboard (677 lines)

**Purpose:** Flask server with SSE real-time streaming and a beautiful dark-glass UI.

**Key Components:**

| Component | Description |
|-----------|-------------|
| `OutputInterceptor` | Thread-aware stdout/stderr replacement for capturing agent text |
| `PipelineHandler` | Per-thread buffer that pushes SSE events to a Queue |
| `make_task_callback()` | CrewAI callback factory for phase transitions |
| `run_pipeline_streaming()` | Background thread: builds crew â†’ kickoff â†’ save â†’ events |
| Routes | 10 routes (pages + API + SSE stream) |
| Template filters | `timeago`, `format_ts` for display formatting |

---

## ğŸ”® Data Flow Diagram

```
 USER                          SYSTEM                          OLLAMA
  â”‚                              â”‚                                â”‚
  â”‚  "Build a todo app"          â”‚                                â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚                                â”‚
  â”‚                              â”‚  Phase 1: Analyst prompt       â”‚
  â”‚                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                              â”‚  â—„â”€â”€â”€ specification.md â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                              â”‚                                â”‚
  â”‚                              â”‚  Phase 2: Developer prompt     â”‚
  â”‚                              â”‚  (includes specification)      â”‚
  â”‚                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                              â”‚  â—„â”€â”€â”€ code.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                              â”‚                                â”‚
  â”‚                              â”‚  Phase 3: Reviewer prompt      â”‚
  â”‚                              â”‚  (includes spec + code)        â”‚
  â”‚                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                              â”‚  â—„â”€â”€â”€ review.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                              â”‚                                â”‚
  â”‚                              â”‚  [If NEEDS_REVISION â†’ loop]    â”‚
  â”‚                              â”‚  Developer.revise(feedback)    â”‚
  â”‚                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                              â”‚  â—„â”€â”€â”€ revised_code.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                              â”‚                                â”‚
  â”‚                              â”‚  Phase 4: QA prompt            â”‚
  â”‚                              â”‚  (includes spec + final code)  â”‚
  â”‚                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                              â”‚  â—„â”€â”€â”€ tests.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                              â”‚                                â”‚
  â”‚                              â”‚  Phase 5: Writer prompt        â”‚
  â”‚                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                              â”‚  â—„â”€â”€â”€ documentation.md â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                              â”‚                                â”‚
  â”‚                              â”‚  Phase 6: DevOps prompt        â”‚
  â”‚                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                              â”‚  â—„â”€â”€â”€ devops.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                              â”‚                                â”‚
  â”‚                              â”‚  Phase 7: UI prompt            â”‚
  â”‚                              â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚
  â”‚                              â”‚  â—„â”€â”€â”€ ui_app.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚                              â”‚                                â”‚
  â”‚  ğŸ“¦ 7 artifacts + manifest   â”‚  Save to crewai_output/<uuid>  â”‚
  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚                                â”‚
```

---

## ğŸ›¡ï¸ Error Handling

### ğŸ”Œ Connection Errors

```python
# Ollama server check (run_crewai_sdlc.py)
try:
    r = requests.get(f"{OLLAMA_BASE_URL}/", timeout=5)
except requests.ConnectionError:
    print("ERROR: Cannot connect to Ollama. Start with: ollama serve")
    sys.exit(1)
```

### ğŸ§  Missing Models

```python
# Verify all required models before pipeline start
required_models = set(m.model.replace("ollama/", "") for m in AGENT_LLMS.values())
installed = {m["name"] for m in requests.get(f"{url}/api/tags").json()["models"]}
missing = [m for m in required_models if not any(m in i for i in installed)]
if missing:
    for m in missing:
        print(f"  ollama pull {m}")
    sys.exit(1)
```

### â±ï¸ Request Timeouts

```python
# BaseAgent: 600s (10 min) timeout per agent call
REQUEST_TIMEOUT = 600

try:
    resp = requests.post(url, json=payload, timeout=REQUEST_TIMEOUT)
except requests.Timeout:
    return f"[ERROR] Request timed out after {REQUEST_TIMEOUT}s"
```

### ğŸŒ Flask SSE Resilience

```python
# Heartbeat every 120s to prevent connection drops
try:
    event = eq.get(timeout=120)
    yield f"data: {json.dumps(event)}\n\n"
except queue.Empty:
    yield 'data: {"event":"heartbeat"}\n\n'
```

### ğŸ§µ Thread Safety

- `OutputInterceptor` uses `threading.Lock()` for handler registration
- `PipelineHandler` uses `threading.Lock()` for buffer access
- `active_runs` dict protected by `runs_lock`
- Each pipeline run gets its own `queue.Queue` for SSE events

---

## ğŸ“ˆ Performance Considerations

### âš¡ Optimization Strategies

| Strategy | Implementation |
|----------|---------------|
| **Sequential Model Loading** | Only one model in VRAM at a time â€” Ollama auto-swaps |
| **Low Temperature** | 0.2-0.3 for deterministic, consistent output |
| **Max Tokens Cap** | 8192 tokens per generation to prevent runaway output |
| **Buffered SSE** | Agent output buffered for 250ms before SSE push |
| **Thread-per-Run** | Each pipeline run is a daemon thread â€” Flask stays responsive |
| **Code Extraction** | Regex-based extraction of python code blocks for clean artifacts |

### ğŸ“Š Typical Resource Usage

```
Pipeline Phase      | Model Loaded       | VRAM     | Duration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€
Requirement Analysis| gpt-oss:20b        | ~13 GB   | 30-60s
Code Generation     | qwen3-coder:30b    | ~18 GB   | 60-120s
Code Review         | devstral-small-2   | ~15 GB   | 30-60s
  â†³ Revision loop   | qwen3-coder + devs | swap     | 60-120s
Test Generation     | qwen3-coder:30b    | ~18 GB   | 60-120s
Documentation       | gpt-oss:20b        | ~13 GB   | 30-60s
DevOps Config       | gpt-oss:20b        | ~13 GB   | 30-60s
UI Design           | gpt-oss:20b        | ~13 GB   | 30-60s
```

---

## ğŸ—ºï¸ Roadmap

### ğŸ”œ Planned Improvements

| Priority | Feature | Description |
|----------|---------|-------------|
| ğŸ”´ High | **Parallel Agent Execution** | Run independent phases concurrently (4-7 can run in parallel) |
| ğŸ”´ High | **Test Execution** | Auto-run generated pytest suite and report results |
| ğŸŸ¡ Medium | **Multi-File Projects** | Support generating multi-file project structures |
| ğŸŸ¡ Medium | **Model Hot-Swap** | Pre-load next model while current phase runs |
| ğŸŸ¡ Medium | **WebSocket Upgrade** | Replace SSE with WebSocket for bidirectional communication |
| ğŸŸ¢ Low | **Custom Agent Prompts** | Allow users to customize agent system prompts |
| ğŸŸ¢ Low | **Plugin System** | Add custom phases (security audit, performance test, etc.) |
| ğŸŸ¢ Low | **Export to GitHub** | Auto-create GitHub repo with all generated artifacts |

---

## ğŸ“„ License & Credits

| Component | License/Source |
|-----------|----------------|
| ğŸ¤– CrewAI | [crewai.com](https://crewai.com) |
| ğŸ§  Ollama | [ollama.com](https://ollama.com) |
| ğŸŒ Flask | BSD License |
| ğŸ¨ UI Theme | Custom dark glass design (2026) |
| ğŸ”¤ Fonts | Inter + JetBrains Mono (Google Fonts) |
| ğŸ­ Icons | Font Awesome 6 |
| ğŸ“ Highlight.js | BSD License |

---

<p align="center">
  <strong>ğŸš€ Built with â¤ï¸ using 7 AI Agents, 3 Local LLMs, and 0 API Keys</strong>
  <br>
  <code>100% Local â€¢ 100% Private â€¢ 100% Powerful</code>
</p>
